{
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n!pip install -qqq --no-deps \"xformers<0.0.24\" \"trl<0.9.0\" peft accelerate bitsandbytes --progress-bar off",
   "metadata": {
    "id": "EPvcJDepawUV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Data Handling and Visualization\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# LLM model training\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom datasets import Dataset\n\n# Saving model\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline",
   "metadata": {
    "id": "IE_MzoFdawUZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = pd.read_json(\"hf://datasets/Amod/mental_health_counseling_conversations/combined_dataset.json\", lines=True)",
   "metadata": {
    "id": "CPATz15KawUa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data.head()",
   "metadata": {
    "id": "NdSS6nzOawUb",
    "outputId": "48d6b7ca-e0e1-48f6-dee3-74fe852e58c7"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 99,
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "3  I'm going through some things with my feelings...   \n",
       "4  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \n",
       "0  If everyone thinks you're worthless, then mayb...  \n",
       "1  Hello, and thank you for your question and see...  \n",
       "2  First thing I'd suggest is getting the sleep y...  \n",
       "3  Therapy is essential for those that are feelin...  \n",
       "4  I first want to let you know that you are not ...  "
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Length of data",
   "metadata": {
    "id": "7FwDILF0awUd"
   }
  },
  {
   "cell_type": "code",
   "source": "print('lenght of data is', len(data))",
   "metadata": {
    "id": "BC_bQYLUawUd",
    "outputId": "94187c3d-2c6b-469f-a9ed-c099980595dd"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "lenght of data is 3512\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Shape of data",
   "metadata": {
    "id": "Hrgy9qMIawUe"
   }
  },
  {
   "cell_type": "code",
   "source": "data.shape",
   "metadata": {
    "id": "VFLIfkYTawUe",
    "outputId": "4a0be774-624d-492c-adca-f94352a05d4b"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 101,
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3512, 2)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Checking Null values",
   "metadata": {
    "id": "F_4nnSj3awUf"
   }
  },
  {
   "cell_type": "code",
   "source": "np.sum(data.isnull().any(axis=1))",
   "metadata": {
    "id": "M6-Ij_kwawUf",
    "outputId": "df4c927c-2650-4ad3-d040-4ad34057c74d",
    "scrolled": true
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 102,
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Rows and columns in the dataset",
   "metadata": {
    "id": "-yp_W7jRawUf"
   }
  },
  {
   "cell_type": "code",
   "source": "print('Count of columns in the data is:  ', len(data.columns))",
   "metadata": {
    "id": "vd228qJtawUg",
    "outputId": "403e9994-1997-4c3c-f15f-14c0b3300728"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Count of columns in the data is:   2\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print('Count of rows in the data is:  ', len(data))",
   "metadata": {
    "id": "tA_G7czfawUg",
    "outputId": "ffa5c0ea-e356-447a-bf8e-d43d2a8a0bff"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Count of rows in the data is:   3512\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Lets check the lenght of words in each context",
   "metadata": {
    "id": "hun-QYyHawUg"
   }
  },
  {
   "cell_type": "code",
   "source": "filtered_data = data[data['Context_length'] <= 1500]",
   "metadata": {
    "id": "ePnToCe2awUi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "filtered_data = filtered_data[ln_Response <= 4000]",
   "metadata": {
    "id": "_6SNm3ZtawUj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f9f9f9; border-left: 6px solid #4CAF50; padding: 10px; margin: 20px 0;\">\n    <strong>Note:</strong> There is no need for such data preparation to handle the lengths of text for LLM models, but for consistency in the number of words, I just took under 4000 words as example so you can do any data preprocessing as per needs.\n</div>\n",
   "metadata": {
    "id": "XZ0XezbuawUk"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<h2 style=\"font-family: Arial, sans-serif; color: #4CAF50; border-bottom: 2px solid #4CAF50; padding-bottom: 5px; margin-bottom: 20px;\">\n   Model training ðŸ§ª\n</h2>",
   "metadata": {
    "id": "X7PPnzVrawUk"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*mjbBdZ1-p7cKQkp-bU7aCg.gif\">",
   "metadata": {
    "id": "GzQADu5PawUk"
   }
  },
  {
   "cell_type": "markdown",
   "source": "\n[Image Reference](https://miro.medium.com/v2/resize:fit:1400/1*mjbBdZ1-p7cKQkp-bU7aCg.gif)\n",
   "metadata": {
    "id": "DV32mkg2awUk"
   }
  },
  {
   "cell_type": "markdown",
   "source": "> Lets deep dive into Llama 3.1 model and train it on our data",
   "metadata": {
    "id": "Q05BfngtawUs"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Loading the model",
   "metadata": {
    "id": "Auokv8pFawUs"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\"> We are going to use Llama 3.1 with 8 billion parameters, but you can use the 70 billion version as well. However, you cannot use the 405 billion parameter version yet with Unsloth because it doesn't support it yet. When support for the 405 billion version is available, you will be able to use it by simply replacing the number!\n</div>\n\n<h3 style=\"color: #388e3c; font-family: Arial, sans-serif;\">Key aspects which can be followed as per your requirement as well:</h3>\n\n<ol style=\"margin-left: 20px;\">\n    <li><strong>Max Sequence Length:</strong>\n        <p>We used <code>max_seq_length</code> 5020, the maximum number of tokens can be used in model that can handle in a single input sequence. This is crucial for tasks requiring the processing of long texts, ensuring that the model can capture more context in each pass. It can be used as per requirements.</p>\n    </li>\n    <li><strong>Loading Llama 3.1 Model:</strong>\n        <p>The model and tokenizer are loaded using <code>FastLanguageModel.from_pretrained</code> with a specific pre-trained model, <code>\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"</code>. This is optimized for 4-bit precision, which reduces memory usage and increases training speed without significantly compromising performance. The <code>load_in_4bit=True</code> parameter enables this efficient 4-bit quantization, making it more suitable for fine-tuning on less powerful hardware.</p>\n    </li>\n    <li><strong>Applying PEFT (Parameter-Efficient Fine-Tuning):</strong>\n        <p>Then we configured model using <code>get_peft_model</code>, which applies LoRA (Low-Rank Adaptation) techniques. This approach focuses on fine-tuning only specific layers or parts of the model, rather than the entire network, drastically reducing the computational resources needed.</p>\n        <p>Parameters such as <code>r=16</code> and <code>lora_alpha=16</code> adjust the complexity and scaling of these adaptations. The use of <code>target_modules</code> specifies which layers of the model should be adapted, which include key components involved in attention mechanisms like <code>q_proj</code>, <code>k_proj</code>, and <code>v_proj</code>.</p>\n        <p><code>use_rslora=True</code> activates Rank-Stabilized LoRA, which improves the stability of the fine-tuning process. <code>use_gradient_checkpointing=\"unsloth\"</code> ensures that memory usage is optimized during training by selectively storing only necessary computations, further enhancing the model's efficiency.</p>\n    </li>\n    <li><strong>Verifying Trainable Parameters:</strong>\n        <p>Finally we are using <code>model.print_trainable_parameters()</code> to print out the number of parameters that will be updated during fine-tuning, allowing to verify that only the intended parts of the model are being trained.</p>\n    </li>\n</ol>\n\n<p>This combination of techniques makes the fine-tuning process not only more efficient but also more accessible, allowing you to deploy this model even with limited computational resources.</p>",
   "metadata": {
    "id": "Gt6qcpmvawUs"
   }
  },
  {
   "cell_type": "markdown",
   "source": "> Setting maximum lenght of tokenz 5020 is more than enough as Low-Rank Adaptation (LoRA) for training but you can use as per your data and requirements.",
   "metadata": {
    "id": "d51SyBp9awUs"
   }
  },
  {
   "cell_type": "code",
   "source": "max_seq_length = 5020\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,\n    dtype=None,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    use_rslora=True,\n    use_gradient_checkpointing=\"unsloth\"\n)\nprint(model.print_trainable_parameters())",
   "metadata": {
    "id": "AwCN3ykMawUt",
    "outputId": "52c51e87-3c1c-4249-9cb6-bca8456f91b5"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.1.\n\n   \\\\   /|    GPU: NVIDIA H100 NVL. Max memory: 93.003 GB. Platform = Linux.\n\nO^O/ \\_/ \\    Pytorch: 2.1.1. CUDA = 9.0. CUDA Toolkit = 12.1.\n\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23.post1. FA2 = False]\n\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n\nNone\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Prapare data for model feed",
   "metadata": {
    "id": "xRQzPI14awUt"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\"> Now its time to design format prompt for mental health analysis. This function analyzes the input text from a psychological perspective, identifying indicators of emotional distress, coping mechanisms, or overall mental well-being. It also highlights potential concerns or positive aspects, providing brief explanations for each observation. We are going to prepare this data for further processing by the model, ensuring that each input-output pair is clearly formatted for effective analysis.\n</div>\n\n<h3 style=\"color: #388e3c; font-family: Arial, sans-serif;\">Main points to remember:</h3>\n\n<ol style=\"margin-left: 20px;\">\n    <li><strong>Data Prompt Structure:</strong>\n        <p>The <code>data_prompt</code> is a formatted string template designed to guide the model in analyzing the provided text. It includes placeholders for the input text (the context) and the model's response. This template specifically prompts the model to identify mental health indicators, making it easier to fine-tune the model for mental health-related tasks.</p>\n    </li>\n    <li><strong>End-of-Sequence Token:</strong>\n        <p>The <code>EOS_TOKEN</code> is retrieved from the tokenizer to signify the end of each text sequence. This token is essential for the model to recognize when a prompt has ended, helping to maintain the structure of the data during training or inference.</p>\n    </li>\n    <li><strong>Formatting Function:</strong>\n        <p>The <code>formatting_prompt</code> used to take a batch of examples and formats them according to the <code>data_prompt</code>. It iterates over the input and output pairs, inserting them into the template and appending the EOS token at the end. The function then returns a dictionary containing the formatted text, ready for model training or evaluation.</p>\n    </li>\n    <li><strong>Function Output:</strong>\n        <p>The function outputs a dictionary where the key is <code>\"text\"</code> and the value is a list of formatted strings. Each string represents a fully prepared prompt for the model, combining the context, response and the structured prompt template.</p>\n    </li>\n</ol>",
   "metadata": {
    "id": "q88hLrOVawUt"
   }
  },
  {
   "cell_type": "code",
   "source": "data_prompt = \"\"\"Analyze the provided text from a mental health perspective. Identify any indicators of emotional distress, coping mechanisms, or psychological well-being. Highlight any potential concerns or positive aspects related to mental health, and provide a brief explanation for each observation.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\ndef formatting_prompt(examples):\n    inputs       = examples[\"Context\"]\n    outputs      = examples[\"Response\"]\n    texts = []\n    for input_, output in zip(inputs, outputs):\n        text = data_prompt.format(input_, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }",
   "metadata": {
    "id": "MtEqWVQOawUu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Format the data for training",
   "metadata": {
    "id": "LEz19boaawUu"
   }
  },
  {
   "cell_type": "code",
   "source": "training_data = Dataset.from_pandas(filtered_data)\ntraining_data = training_data.map(formatting_prompt, batched=True)",
   "metadata": {
    "id": "Nd9zCTBFawUu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Model training with custom parameters and data",
   "metadata": {
    "id": "XNjTEgLXawUv"
   }
  },
  {
   "cell_type": "markdown",
   "source": "> <p style=\"font-size: 14px; color: #333;\">\n        Using <code>sudo apt-get update</code> to refresh the list of available packages and <code>sudo apt-get install build-essential</code> to install essential tools. Only run this on shell if you get any error.\n    </p>\n",
   "metadata": {
    "id": "BfSLRJZwawUv"
   }
  },
  {
   "cell_type": "code",
   "source": "#sudo apt-get update\n#sudo apt-get install build-essential",
   "metadata": {
    "id": "gObTzUbkawUv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<h3 style=\"color: #388e3c; font-family: Arial, sans-serif;\">Training setup to start fine tuning!</h3>\n\n<ol style=\"margin-left: 20px;\">\n    <li><strong>Trainer Initialization:</strong>\n        <p>We are going to initialize <code>SFTTrainer</code> with the model and tokenizer, as well as the training dataset. The <code>dataset_text_field</code> parameter specifies the field in the dataset that contains the text to be used for training which we prepared above. The trainer is responsible for managing the fine-tuning process, including data handling and model updates.</p>\n    </li>\n    <li><strong>Training Arguments:</strong>\n        <p>The <code>TrainingArguments</code> class is used to define key hyperparameters for the training process. These include:</p>\n        <ul>\n            <li><code>learning_rate=3e-4</code>: Sets the learning rate for the optimizer.</li>\n            <li><code>per_device_train_batch_size=32</code>: Defines the batch size per device, optimizing GPU usage.</li>\n            <li><code>num_train_epochs=20</code>: Specifies the number of training epochs.</li>\n            <li><code>fp16=not is_bfloat16_supported()</code> and <code>bf16=is_bfloat16_supported()</code>: Enable mixed precision training to reduce memory usage, depending on hardware support.</li>\n            <li><code>optim=\"adamw_8bit\"</code>: Uses the 8-bit AdamW optimizer for efficient memory usage.</li>\n            <li><code>weight_decay=0.01</code>: Applies weight decay to prevent overfitting.</li>\n            <li><code>output_dir=\"output\"</code>: Specifies the directory where the trained model and logs will be saved.</li>\n        </ul>\n    </li>\n    <li><strong>Training Process:</strong>\n        <p>Finally we called <code>trainer.train()</code> method to start the training process. It uses the defined parameters of our fine-tune the model, adjusting weights and learning from the provided dataset. The trainer also handles data packing and gradient accumulation, optimizing the training pipeline for better performance.</p>\n    </li>\n</ol>",
   "metadata": {
    "id": "VTeMR1lvawUv"
   }
  },
  {
   "cell_type": "code",
   "source": "trainer=SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=training_data,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=3e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=32,\n        gradient_accumulation_steps=4,\n        num_train_epochs=20,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        seed=0,\n    ),\n)\n\ntrainer.train()",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "79f9e484ae554ab98df169f794b2cda5"
     ]
    },
    "id": "30dJa7yyawUw",
    "outputId": "d2083d14-962a-438e-a854-515d061bbecf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f9e484ae554ab98df169f794b2cda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n\n   \\\\   /|    Num examples = 593 | Num Epochs = 20\n\nO^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n\n\\        /    Total batch size = 128 | Total steps = 80\n\n \"-____-\"     Number of trainable parameters = 41,943,040\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 49:24, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.840600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.868500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=1.4284833692014218, metrics={'train_runtime': 3007.0031, 'train_samples_per_second': 3.944, 'train_steps_per_second': 0.027, 'total_flos': 9.2735910445056e+17, 'train_loss': 1.4284833692014218, 'epoch': 16.842105263157894})"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Inference",
   "metadata": {
    "id": "6-qIx5ZdawUw"
   }
  },
  {
   "cell_type": "code",
   "source": "text=\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"",
   "metadata": {
    "id": "zxl-iBhRawUx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\">\n    <strong>Note:</strong> Lets use the fine-tuned model for inference in order to generate responses based on mental health-related prompts !\n</div>\n\n<h3 style=\"color: #388e3c; font-family: Arial, sans-serif;\">Here is some keys to note:</h3>\n\n<ol style=\"margin-left: 20px;\">\n        <p>The <code>model = FastLanguageModel.for_inference(model)</code> configures the model specifically for inference, optimizing its performance for generating responses.</p>\n    </li>\n        <p>The input text is tokenized using the <code>tokenizer</code>, it convert the text into a format that model can process. We are using <code>data_prompt</code> to format the input text, while the response placeholder is left empty to get response from model. The <code>return_tensors = \"pt\"</code> parameter specifies that the output should be in PyTorch tensors, which are then moved to the GPU using <code>.to(\"cuda\")</code> for faster processing.</p>\n    </li>\n        <p>The <code>model.generate</code> method generating response based on the tokenized inputs. The parameters <code>max_new_tokens = 5020</code> and <code>use_cache = True</code> ensure that the model can produce long and coherent responses efficiently by utilizing cached computation from previous layers.</p>\n    </li>\n</ol>",
   "metadata": {
    "id": "cT3M9mzOawUx"
   }
  },
  {
   "cell_type": "code",
   "source": "model = FastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    data_prompt.format(\n        #instructions\n        text,\n        #answer\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\nanswer=tokenizer.batch_decode(outputs)\nanswer = answer[0].split(\"### Response:\")[-1]\nprint(\"Answer of the question is:\", answer)",
   "metadata": {
    "id": "O0qk3PutawUx",
    "outputId": "881f4a0d-9dde-4581-ede3-06537bd8ee4c"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Answer of the question is: \n\nHi there, I want to acknowledge you for reaching out to ask for help. That's the first step! It sounds like you are experiencing some depression symptoms and it is important to seek help for them as they can worsen over time. You are not worthless and this thought is only a thought, it's not the truth. A professional counsellor can help you change negative thought patterns, build coping skills and learn to challenge negative thoughts.Â I recommend you to contact a professional counsellor, your family doctor or a crisis hot-line to discuss your symptoms. You are not alone and you can get help! Take care.Â Rosanne Bermudez, CCVP, RCCWSÂ <|im_end|>\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\">   <strong>Note:</strong> Here is how we can securely push a fine-tuned model and its tokenizer to the Hugging Face Hub so any body can use it. It can be accessed on my account <strong><a href=\"https://huggingface.co/ImranzamanML/finetuned_llama3.1\" target=\"_blank\">ImranzamanML/finetuned_llama3.1</a></strong>\n</div>\n",
   "metadata": {
    "id": "6Sohb0kxawUy"
   }
  },
  {
   "cell_type": "code",
   "source": "os.environ[\"HF_TOKEN\"] = \"hugging face token key, you can create from your HF account.\"\nmodel.push_to_hub(\"ImranzamanML/finetuned_llama3.1\", use_auth_token=os.getenv(\"HF_TOKEN\"))\ntokenizer.push_to_hub(\"ImranzamanML/finetuned_llama3.1\", use_auth_token=os.getenv(\"HF_TOKEN\"))",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b0aec8988cbf4206b0e6cdfcfcf15fc0",
      "6415fa4e334c44b6913bf9a2db4e1452"
     ]
    },
    "id": "OEBmHxlxawUy",
    "outputId": "a8de077b-98ce-49c8-d545-ef2436e341e9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aec8988cbf4206b0e6cdfcfcf15fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6415fa4e334c44b6913bf9a2db4e1452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Saved model to https://huggingface.co/ImranzamanML/finetuned_llama3.1\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\">   <strong>Note:</strong> We can also save fine-tuned model and its tokenizer locally on the machine.\n</div>\n",
   "metadata": {
    "id": "40wFvLmhawUy"
   }
  },
  {
   "cell_type": "code",
   "source": "model.save_pretrained(\"model/finetuned_llama3.1\")\ntokenizer.save_pretrained(\"model/finetuned_llama3.1\")",
   "metadata": {
    "id": "TtoC1t66awUz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n        \n<div style=\"background-color: #f2f2f2; border-left: 5px solid #4CAF50; padding: 15px; margin: 20px 0;\"> Still ðŸ‘€ for something?\n</div>\nOk let me show you how you can load your saved model and use it!\n\n<pre style=\"background-color: #e0f7fa; border: 1px solid #009688; padding: 10px; border-radius: 5px; font-family: Consolas, 'Courier New', monospace; font-size: 14px;\">\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = \"model/finetuned_llama3.1\",\nmax_seq_length = 5020,\ndtype = None,\nload_in_4bit = True)\n</pre>\n",
   "metadata": {
    "id": "xi13VN69awUz"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<p style=\"font-family: Comic Sans MS, cursive, sans-serif; color: #333; font-size: 16px;\">\n    No way, still searching for something? ðŸ˜„ No worries! You can use the prompt format and code above to get response for mental peace ðŸ§ âœ¨\n</p>\n",
   "metadata": {
    "id": "WkgDfYgVawUz"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Happy to Connect ðŸ˜Š\n<div style=\"background-color: #f9f9f9; border: 1px solid #ddd; padding: 20px; text-align: center; font-family: Arial, sans-serif;\">\n    <h3 style=\"margin-bottom: 20px;\">Muhammad Imran Zaman</h3>\n    <a href=\"https://www.kaggle.com/muhammadimran112233\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"https://www.kaggle.com/static/images/site-logo.svg\" width=\"100\" height=\"100\" alt=\"Kaggle Profile\">\n    </a>\n    <a href=\"https://www.linkedin.com/in/muhammad-imran-zaman\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/LinkedIn_logo_initials.png/600px-LinkedIn_logo_initials.png\" width=\"50\" height=\"50\" alt=\"LinkedIn Profile\">\n    </a>\n    <a href=\"https://scholar.google.com/citations?user=ulVFpy8AAAAJ&hl=en\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Google_Scholar_logo.svg/768px-Google_Scholar_logo.svg.png\" width=\"50\" height=\"50\" alt=\"Google Scholar Profile\">\n    </a>\n    <a href=\"https://www.youtube.com/@consolioo\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/YouTube_social_white_square_%282017%29.svg/640px-YouTube_social_white_square_%282017%29.svg.png\" width=\"50\" height=\"50\" alt=\"YouTube Channel\">\n    </a>\n    <a href=\"https://github.com/Imran-ml\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" height=\"50\" alt=\"GitHub Profile\">\n    </a>\n    <a href=\"https://imranzaman-5202.medium.com/\" style=\"text-decoration: none; margin-right: 20px;\">\n        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOEAAADhCAMAAAAJbSJIAAAAeFBMVEUAAAD///+3t7f8/Pzs7Oz39/d6enpPT0/IyMjy8vL19fWtra04ODhxcXHU1NSqqqpeXl6Xl5fc3NwJCQl/f38WFhbm5uaPj4/AwMArKyuFhYVNTU1kZGQREREyMjJFRUUiIiKgoKBoaGgnJyc9PT2MjIwcHBxWVlbgTdlVAAAEsUlEQVR4nO3c6XqqMBAGYHDD3dbj1rpUu9j7v8NTCAhhnYQZofZ7f/LEmGHJRoLjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQJtdp95hNpsd9qdj00Xh9zHoLEdubDSfDP41XSg+h04yuFhv4TVdNA6nST83vDDIy9Usu9VuMx7NF98yhbVwmJeEp+zeyLkdO/GpWQmWmm6/rIzPNzzRsltpv1qeRctOsR6S4vNNKPl10r96ko6gwis5vh/jQ2V+Off7IC/ddL67T2NU/QDqOhX55d4QOVfx2nXdjUhEOq+sAs3XeynLcJL/o2kmYXAmLmKBRQbG8flKmoD3otOSTngKDnclg/NdrAIseK5868KfpG/u8Fq/yga4sAzQdYsauZJaOdUtGqujc9EAL9YBFl3Fp5Jf6LWKFx1eCwb4XCNA133Py3JT9gvthrzdPoU3fH2zWgG6bk4frjzLfjLprRNV1fjY+6wZoF5gpfQS6g/v7WCmkmWTP04ykakkyp5CX+JJ9OKjUqPPTN/RwnMqz8rubdyObuODQp3WqtNNo3duitvCyC7vBKfPExPzvlqepZbntvoHt7SJ0ZpMVXNhCTBV1VfUM1r6xBleZotX35UpQK0+fSGkH0aJE8dGEhFyVDNK4iH6IiSPOtpe8qBAgGyXULuIpImCsOLUhjSf/BEWDOGsxF2xLiX5QqXVOowz/ghJZSG6dUneSMnDakV7TL7YA1wV/b2VaFBEaCt8KrF2R/OP800nZspFzdmOllydEG3ycldcVDvVXQ8jUV3ToyVXz63WKR4Wl9WO0eQhQXibEh9uNeGqdanYm3z69C+Nqh6pg7F59nSwj5/GzBGqS1A0x5YWhHPWDnHPt52YAwyrR/KUiJ94Khohb1vhCx5E8rTd2cm0ncwRcnZolKB6JD/deyfVLWWfbuOuaMLqkfZ6zlUDqNT4u/QtgTnCKM7Q0Chb/4of9EPMXW/OTqkSTDCRJ7a2TmbWkf5uuaEIe0bZ+r3Q1PugPWuA5/z/rSMYpJNTL6QjpMw1GOrXjZB3KYtAhN2Hj7Bl1/Dxn0N6UchaVpf+gfawDX2a1FAru06jlqb7pf7coWy/1H51QhGzsYVfccqOLfjHh99GJ+7sSI8P+RvEINsWjfEff56m4bk2dT5k59q450vDl/Mtmi9tds5bvYaRnfNu9r2F6qHJvrf4A++e2vb+UGAB5uO/A2YcI3K8x2fueAf41mJs40zbtBaD7yK2dj2Nc2GKsLVromTWtRE637e04uva0m8OLOnboI6V6XPXJm7TZWPSyPrSeCmp/PpScj+yBNMaYakAU6NQC61f500esxYxXquvNQt3WKt///0W2j6bS3RUcL9FS/bMyG5BtA+Rbd+TUHt/Y3uj/pq9a7Zbgzj2H4Z1uWRwyv7R95A6ze0DPt5pH7BjOL3Yt9rLnTuMn86H9/rixpE4T+b+1v34PzzRbyp8yBae6Jv1uxjXxHcxRDstRk6Tsnc2I9Nvm7zuNv3RvG2ffXkq+j7NpGUFreO8euhvDEXWU+89+E7U9AG/EwUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ/wFTZUNtMEP4sAAAAABJRU5ErkJggg==\" width=\"50\" height=\"50\" alt=\"Medium Profile\">\n    </a>\n    <a href=\"https://huggingface.co/ImranzamanML\" style=\"text-decoration: none;\">\n        <img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.svg\" width=\"200\" height=\"100\" alt=\"Hugging Face Profile\">\n    </a>\n</div>\n",
   "metadata": {}
  }
 ]
}
